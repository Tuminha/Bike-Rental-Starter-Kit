{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üö¥‚Äç‚ôÇÔ∏è Bike Rental Analytics - Phase 1: Data Exploration\n",
        "\n",
        "## Project Overview\n",
        "This notebook explores Citi Bike ridership data from Jersey City (2016) combined with NOAA weather data from Newark Airport to understand weather impact on bike rentals.\n",
        "\n",
        "## Phase 1 Objectives\n",
        "- Load and inspect Citi Bike CSV files\n",
        "- Examine weather data structure and quality\n",
        "- Identify data quality issues and patterns\n",
        "- Document initial findings and assumptions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "\n",
        "# Set up plotting styles\n",
        "sns.set_theme(style=\"whitegrid\", palette=\"muted\", font_scale=1.1)\n",
        "plt.style.use(\"ggplot\")\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÅ Data Loading\n",
        "\n",
        "Let's start by exploring the data structure and loading our datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV files found:\n",
            "JC-201602-citibike-tripdata.csv\n",
            "JC-201608-citibike-tripdata.csv\n",
            "newark_airport_2016.csv\n",
            "JC-201606-citibike-tripdata.csv\n",
            "JC-201610-citibike-tripdata.csv\n",
            "JC-201604-citibike-tripdata.csv\n",
            "JC-201612-citibike-tripdata.csv\n",
            "JC-201603-citibike-tripdata.csv\n",
            "JC-201609-citibike-tripdata.csv\n",
            "JC-201601-citibike-tripdata.csv\n",
            "JC-201611-citibike-tripdata.csv\n",
            "JC-201607-citibike-tripdata.csv\n",
            "JC-201605-citibike-tripdata.csv\n"
          ]
        }
      ],
      "source": [
        "# TODO: Explore the data directory structure\n",
        "# Hint: Use Path('../data') to create a path object, then use .glob('*.csv') to find CSV files\n",
        "# Print out all the CSV files you find\n",
        "data_path = Path(\"../data\")\n",
        "data_csv = list(data_path.glob(\"*.csv\"))\n",
        "print(\"CSV files found:\")\n",
        "for csv_file in data_csv:\n",
        "    print(csv_file.name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üö¥‚Äç‚ôÇÔ∏è Citi Bike Data Exploration\n",
        "\n",
        "Let's start by examining one month of Citi Bike data to understand the structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Trip Duration           Start Time            Stop Time  Start Station ID  \\\n",
            "0            362  2016-01-01 00:02:52  2016-01-01 00:08:54              3186   \n",
            "1            200  2016-01-01 00:18:22  2016-01-01 00:21:42              3186   \n",
            "2            202  2016-01-01 00:18:25  2016-01-01 00:21:47              3186   \n",
            "3            248  2016-01-01 00:23:13  2016-01-01 00:27:21              3209   \n",
            "4            903  2016-01-01 01:03:20  2016-01-01 01:18:24              3195   \n",
            "\n",
            "  Start Station Name  Start Station Latitude  Start Station Longitude  \\\n",
            "0      Grove St PATH               40.719586               -74.043117   \n",
            "1      Grove St PATH               40.719586               -74.043117   \n",
            "2      Grove St PATH               40.719586               -74.043117   \n",
            "3       Brunswick St               40.724176               -74.050656   \n",
            "4            Sip Ave               40.730743               -74.063784   \n",
            "\n",
            "   End Station ID End Station Name  End Station Latitude  \\\n",
            "0            3209     Brunswick St             40.724176   \n",
            "1            3213   Van Vorst Park             40.718489   \n",
            "2            3213   Van Vorst Park             40.718489   \n",
            "3            3203    Hamilton Park             40.727596   \n",
            "4            3210   Pershing Field             40.742677   \n",
            "\n",
            "   End Station Longitude  Bike ID   User Type  Birth Year  Gender  \n",
            "0             -74.050656    24647  Subscriber      1964.0       2  \n",
            "1             -74.047727    24605  Subscriber      1962.0       1  \n",
            "2             -74.047727    24689  Subscriber      1962.0       2  \n",
            "3             -74.044247    24693  Subscriber      1984.0       1  \n",
            "4             -74.051789    24573    Customer         NaN       0  \n",
            "\n",
            "Data shape:\n",
            "(7479, 15)\n",
            "Data has 7479 rows and 15 columns\n",
            "\n",
            "Column names:\n",
            "Index(['Trip Duration', 'Start Time', 'Stop Time', 'Start Station ID',\n",
            "       'Start Station Name', 'Start Station Latitude',\n",
            "       'Start Station Longitude', 'End Station ID', 'End Station Name',\n",
            "       'End Station Latitude', 'End Station Longitude', 'Bike ID', 'User Type',\n",
            "       'Birth Year', 'Gender'],\n",
            "      dtype='object')\n",
            "\n",
            "Type of data:\n",
            "Trip Duration                int64\n",
            "Start Time                  object\n",
            "Stop Time                   object\n",
            "Start Station ID             int64\n",
            "Start Station Name          object\n",
            "Start Station Latitude     float64\n",
            "Start Station Longitude    float64\n",
            "End Station ID               int64\n",
            "End Station Name            object\n",
            "End Station Latitude       float64\n",
            "End Station Longitude      float64\n",
            "Bike ID                      int64\n",
            "User Type                   object\n",
            "Birth Year                 float64\n",
            "Gender                       int64\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# TODO: Load the January 2016 Citi Bike data\n",
        "# Hint: Use pd.read_csv() to load 'JC-201601-citibike-tripdata.csv'\n",
        "# Print the shape, number of records, and column names\n",
        "# What do you notice about the data structure?\n",
        "january_data = pd.read_csv(\"../data/JC-201601-citibike-tripdata.csv\")\n",
        "print(january_data.head())\n",
        "print()\n",
        "print(\"Data shape:\")\n",
        "print(january_data.shape)\n",
        "print(f\"Data has {january_data.shape[0]} rows and {january_data.shape[1]} columns\")\n",
        "print()\n",
        "print(\"Column names:\")\n",
        "print(january_data.columns)\n",
        "print()\n",
        "print(\"Type of data:\")\n",
        "print(january_data.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       STATION                                         NAME        DATE  \\\n",
            "0  USW00014734  NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US  2016-01-01   \n",
            "1  USW00014734  NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US  2016-01-02   \n",
            "2  USW00014734  NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US  2016-01-03   \n",
            "3  USW00014734  NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US  2016-01-04   \n",
            "4  USW00014734  NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US  2016-01-05   \n",
            "\n",
            "    AWND  PGTM  PRCP  SNOW  SNWD  TAVG  TMAX  TMIN  TSUN  WDF2   WDF5  WSF2  \\\n",
            "0  12.75   NaN   0.0   0.0   0.0    41    43    34   NaN   270  280.0  25.9   \n",
            "1   9.40   NaN   0.0   0.0   0.0    36    42    30   NaN   260  260.0  21.0   \n",
            "2  10.29   NaN   0.0   0.0   0.0    37    47    28   NaN   270  250.0  23.9   \n",
            "3  17.22   NaN   0.0   0.0   0.0    32    35    14   NaN   330  330.0  25.9   \n",
            "4   9.84   NaN   0.0   0.0   0.0    19    31    10   NaN   360  350.0  25.1   \n",
            "\n",
            "   WSF5  \n",
            "0  35.1  \n",
            "1  25.1  \n",
            "2  30.0  \n",
            "3  33.1  \n",
            "4  31.1  \n"
          ]
        }
      ],
      "source": [
        "# TODO: Examine the first few rows of the Citi Bike data\n",
        "# Hint: Use .head() method to see the first 5 rows\n",
        "# What patterns do you see in the data?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Get basic information about the Citi Bike data\n",
        "# Hint: Use .info() to see data types and .describe() for statistics\n",
        "# What data types do you see? Are there any obvious issues?\n",
        "\n",
        "# Your code here:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üå§Ô∏è Weather Data Exploration\n",
        "\n",
        "Now let's examine the weather data structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       STATION                                         NAME        DATE  \\\n",
            "0  USW00014734  NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US  2016-01-01   \n",
            "1  USW00014734  NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US  2016-01-02   \n",
            "2  USW00014734  NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US  2016-01-03   \n",
            "3  USW00014734  NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US  2016-01-04   \n",
            "4  USW00014734  NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US  2016-01-05   \n",
            "\n",
            "    AWND  PGTM  PRCP  SNOW  SNWD  TAVG  TMAX  TMIN  TSUN  WDF2   WDF5  WSF2  \\\n",
            "0  12.75   NaN   0.0   0.0   0.0    41    43    34   NaN   270  280.0  25.9   \n",
            "1   9.40   NaN   0.0   0.0   0.0    36    42    30   NaN   260  260.0  21.0   \n",
            "2  10.29   NaN   0.0   0.0   0.0    37    47    28   NaN   270  250.0  23.9   \n",
            "3  17.22   NaN   0.0   0.0   0.0    32    35    14   NaN   330  330.0  25.9   \n",
            "4   9.84   NaN   0.0   0.0   0.0    19    31    10   NaN   360  350.0  25.1   \n",
            "\n",
            "   WSF5  \n",
            "0  35.1  \n",
            "1  25.1  \n",
            "2  30.0  \n",
            "3  33.1  \n",
            "4  31.1  \n",
            "\n",
            "Columns names:\n",
            "Index(['STATION', 'NAME', 'DATE', 'AWND', 'PGTM', 'PRCP', 'SNOW', 'SNWD',\n",
            "       'TAVG', 'TMAX', 'TMIN', 'TSUN', 'WDF2', 'WDF5', 'WSF2', 'WSF5'],\n",
            "      dtype='object')\n",
            "\n",
            "Data shape:\n",
            "(366, 16)\n",
            "\n",
            "Types of data:\n",
            "STATION     object\n",
            "NAME        object\n",
            "DATE        object\n",
            "AWND       float64\n",
            "PGTM       float64\n",
            "PRCP       float64\n",
            "SNOW       float64\n",
            "SNWD       float64\n",
            "TAVG         int64\n",
            "TMAX         int64\n",
            "TMIN         int64\n",
            "TSUN       float64\n",
            "WDF2         int64\n",
            "WDF5       float64\n",
            "WSF2       float64\n",
            "WSF5       float64\n",
            "dtype: object\n",
            "\n",
            "Describe data:\n",
            "             AWND  PGTM        PRCP        SNOW        SNWD        TAVG  \\\n",
            "count  366.000000   0.0  366.000000  366.000000  366.000000  366.000000   \n",
            "mean     9.429973   NaN    0.104945    0.098087    0.342623   57.196721   \n",
            "std      3.748174   NaN    0.307496    1.276498    2.078510   17.466981   \n",
            "min      2.460000   NaN    0.000000    0.000000    0.000000    8.000000   \n",
            "25%      6.765000   NaN    0.000000    0.000000    0.000000   43.000000   \n",
            "50%      8.720000   NaN    0.000000    0.000000    0.000000   56.000000   \n",
            "75%     11.410000   NaN    0.030000    0.000000    0.000000   74.000000   \n",
            "max     22.820000   NaN    2.790000   24.000000   20.100000   89.000000   \n",
            "\n",
            "             TMAX        TMIN  TSUN        WDF2        WDF5        WSF2  \\\n",
            "count  366.000000  366.000000   0.0  366.000000  364.000000  366.000000   \n",
            "mean    65.991803   48.459016   NaN  217.841530  228.269231   20.484426   \n",
            "std     18.606301   17.135790   NaN  102.548282   97.415777    6.848390   \n",
            "min     18.000000    0.000000   NaN   10.000000   10.000000    6.900000   \n",
            "25%     51.250000   35.000000   NaN  150.000000  150.000000   15.000000   \n",
            "50%     66.000000   47.000000   NaN  240.000000  260.000000   19.900000   \n",
            "75%     83.000000   64.000000   NaN  300.000000  300.000000   23.900000   \n",
            "max     99.000000   80.000000   NaN  360.000000  360.000000   48.100000   \n",
            "\n",
            "             WSF5  \n",
            "count  364.000000  \n",
            "mean    26.801648  \n",
            "std      8.882610  \n",
            "min     10.100000  \n",
            "25%     19.900000  \n",
            "50%     25.100000  \n",
            "75%     31.100000  \n",
            "max     66.000000  \n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 366 entries, 0 to 365\n",
            "Data columns (total 16 columns):\n",
            " #   Column   Non-Null Count  Dtype  \n",
            "---  ------   --------------  -----  \n",
            " 0   STATION  366 non-null    object \n",
            " 1   NAME     366 non-null    object \n",
            " 2   DATE     366 non-null    object \n",
            " 3   AWND     366 non-null    float64\n",
            " 4   PGTM     0 non-null      float64\n",
            " 5   PRCP     366 non-null    float64\n",
            " 6   SNOW     366 non-null    float64\n",
            " 7   SNWD     366 non-null    float64\n",
            " 8   TAVG     366 non-null    int64  \n",
            " 9   TMAX     366 non-null    int64  \n",
            " 10  TMIN     366 non-null    int64  \n",
            " 11  TSUN     0 non-null      float64\n",
            " 12  WDF2     366 non-null    int64  \n",
            " 13  WDF5     364 non-null    float64\n",
            " 14  WSF2     366 non-null    float64\n",
            " 15  WSF5     364 non-null    float64\n",
            "dtypes: float64(9), int64(4), object(3)\n",
            "memory usage: 45.9+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# TODO: Load the weather data\n",
        "# Hint: Use pd.read_csv() to load 'newark_airport_2016.csv'\n",
        "# Print the shape, number of records, and column names\n",
        "# How does this data structure compare to the Citi Bike data?\n",
        "\n",
        "# Your c# Your code here:\n",
        "newark_airport_df = pd.read_csv(\"../data/newark_airport_2016.csv\")\n",
        "\n",
        "print(newark_airport_df.head())\n",
        "print()\n",
        "print(\"Columns names:\")\n",
        "print(newark_airport_df.columns)\n",
        "print()\n",
        "print(\"Data shape:\")\n",
        "print(newark_airport_df.shape)\n",
        "\n",
        "print()\n",
        "print(\"Types of data:\")\n",
        "print(newark_airport_df.dtypes)\n",
        "print()\n",
        "print(\"Describe data:\")\n",
        "print(newark_airport_df.describe())\n",
        "print()\n",
        "\n",
        "print(newark_airport_df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Examine the weather data structure\n",
        "# Hint: Use .head(), .info(), and .describe() methods\n",
        "# What weather variables are available? What's the time granularity?\n",
        "\n",
        "# Your code here:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Initial Data Quality Assessment\n",
        "\n",
        "Let's identify potential data quality issues that we'll need to address in Phase 2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of missing values:\n",
            "Trip Duration                  0\n",
            "Start Time                     0\n",
            "Stop Time                      0\n",
            "Start Station ID               0\n",
            "Start Station Name             0\n",
            "Start Station Latitude         0\n",
            "Start Station Longitude        0\n",
            "End Station ID                 0\n",
            "End Station Name               0\n",
            "End Station Latitude           0\n",
            "End Station Longitude          0\n",
            "Bike ID                        0\n",
            "User Type                    380\n",
            "Birth Year                 18999\n",
            "Gender                         0\n",
            "dtype: int64\n",
            "\n",
            "Percentage of missing values per column:\n",
            "Trip Duration              0.00\n",
            "Start Time                 0.00\n",
            "Stop Time                  0.00\n",
            "Start Station ID           0.00\n",
            "Start Station Name         0.00\n",
            "Start Station Latitude     0.00\n",
            "Start Station Longitude    0.00\n",
            "End Station ID             0.00\n",
            "End Station Name           0.00\n",
            "End Station Latitude       0.00\n",
            "End Station Longitude      0.00\n",
            "Bike ID                    0.00\n",
            "User Type                  0.15\n",
            "Birth Year                 7.67\n",
            "Gender                     0.00\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Is this correct? Let's review and comment.\n",
        "\n",
        "# The code below loads all Citi Bike data for 2016, concatenates it, and checks for missing values.\n",
        "# This approach is correct for checking missing data across all months.\n",
        "\n",
        "import glob\n",
        "\n",
        "# Get all monthly files for 2016\n",
        "citibike_files = sorted(glob.glob(\"../data/JC-2016*-citibike-tripdata.csv\"))\n",
        "\n",
        "# Read each file into a DataFrame and concatenate them\n",
        "all_months_data = [pd.read_csv(f) for f in citibike_files]\n",
        "citibike_2016_data = pd.concat(all_months_data, ignore_index=True)\n",
        "\n",
        "# Check for missing values using .isnull().sum()\n",
        "number_nulls_citibike = citibike_2016_data.isnull().sum()\n",
        "print(\"Number of missing values:\")\n",
        "print(number_nulls_citibike)\n",
        "\n",
        "# .isnull() and .isna() are equivalent in pandas, so the next line is redundant.\n",
        "# You only need to use one of them.\n",
        "# number_nan_citibike = citibike_2016_data.isna().sum()\n",
        "# print(number_nan_citibike)\n",
        "\n",
        "# To better understand the impact, let's also calculate the percentage of missing values per column:\n",
        "print(\"\\nPercentage of missing values per column:\")\n",
        "percent_nulls = round((number_nulls_citibike / len(citibike_2016_data)) * 100, 2)\n",
        "print(percent_nulls)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing numbers in weather df:\n",
            "STATION      0\n",
            "NAME         0\n",
            "DATE         0\n",
            "AWND         0\n",
            "PGTM       366\n",
            "PRCP         0\n",
            "SNOW         0\n",
            "SNWD         0\n",
            "TAVG         0\n",
            "TMAX         0\n",
            "TMIN         0\n",
            "TSUN       366\n",
            "WDF2         0\n",
            "WDF5         2\n",
            "WSF2         0\n",
            "WSF5         2\n",
            "dtype: int64\n",
            "STATION      0.00\n",
            "NAME         0.00\n",
            "DATE         0.00\n",
            "AWND         0.00\n",
            "PGTM       100.00\n",
            "PRCP         0.00\n",
            "SNOW         0.00\n",
            "SNWD         0.00\n",
            "TAVG         0.00\n",
            "TMAX         0.00\n",
            "TMIN         0.00\n",
            "TSUN       100.00\n",
            "WDF2         0.00\n",
            "WDF5         0.55\n",
            "WSF2         0.00\n",
            "WSF5         0.55\n",
            "dtype: float64\n",
            "\n",
            "Data shape: (366, 16)\n"
          ]
        }
      ],
      "source": [
        "# TODO: Check for missing values in Weather data\n",
        "# Hint: Use .isnull().sum() to count missing values\n",
        "# Calculate the percentage of missing values\n",
        "# Are there any missing weather observations?\n",
        "\n",
        "# Your code here:\n",
        "newark_missing_values = newark_airport_df.isnull().sum()\n",
        "print(\"Missing numbers in weather df:\")\n",
        "print(newark_missing_values)\n",
        "missing_weather_percentage = round((newark_missing_values / len(newark_airport_df)) * 100, 2)\n",
        "print(missing_weather_percentage)\n",
        "print()\n",
        "print(\"Data shape:\", newark_airport_df.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Quick Exploratory Analysis\n",
        "\n",
        "Now let's do some basic analysis to understand the data better.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trip duration patterns:\n",
            "count    2.475840e+05\n",
            "mean     8.856305e+02\n",
            "std      3.593798e+04\n",
            "min      6.100000e+01\n",
            "25%      2.480000e+02\n",
            "50%      3.900000e+02\n",
            "75%      6.660000e+02\n",
            "max      1.632981e+07\n",
            "Name: Trip Duration, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# TODO: Analyze trip duration patterns\n",
        "# Hint: Use .describe() on the 'Trip Duration' column\n",
        "# What's the average trip duration? Are there any extreme values?\n",
        "# Think about: What might cause very short or very long trips?\n",
        "\n",
        "# Your code here:\n",
        "trip_duration_patterns = citibike_2016_data[\"Trip Duration\"].describe()\n",
        "print(\"Trip duration patterns:\")\n",
        "print(trip_duration_patterns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User Type\n",
            "Subscriber    231683\n",
            "Customer       15521\n",
            "Name: count, dtype: int64\n",
            "Gender\n",
            "1    177197\n",
            "2     50486\n",
            "0     19901\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# TODO: Look at user types and gender distribution\n",
        "# Hint: Use .value_counts() on 'User Type' and 'Gender' columns\n",
        "# What types of users are there? How is gender coded?\n",
        "print(citibike_2016_data[\"User Type\"].value_counts())\n",
        "print(citibike_2016_data[\"Gender\"].value_counts())\n",
        "\n",
        "# Your code here:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max Citibike date: 2016-12-31\n",
            "Min Citibike date: 2016-01-01\n",
            "\n",
            "Max weather date: 2016-12-31\n",
            "Min weather date: 2016-01-01\n"
          ]
        }
      ],
      "source": [
        "# TODO: Examine the date ranges in both datasets\n",
        "# Hint: Convert date columns to datetime and check min/max dates\n",
        "# For Citi Bike: look at 'Start Time' column\n",
        "# For Weather: look at 'DATE' column\n",
        "# Do the date ranges overlap? What's the coverage?\n",
        "# Split the 'Start Time' string into date and time using .str.split(), then take the date part\n",
        "citibike_2016_data[\"Date\"] = citibike_2016_data[\"Start Time\"].str.split(\" \").str[0]\n",
        "citibike_2016_data[\"Time\"] = citibike_2016_data[\"Start Time\"].str.split(\" \").str[1]\n",
        "print(\"Max Citibike date:\", citibike_2016_data[\"Date\"].max())\n",
        "print(\"Min Citibike date:\", citibike_2016_data[\"Date\"].min())\n",
        "print()\n",
        "print(\"Max weather date:\", newark_airport_df[\"DATE\"].max())\n",
        "print(\"Min weather date:\", newark_airport_df[\"DATE\"].min())\n",
        "\n",
        "\n",
        "\n",
        "# Your code here:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Your Initial Findings\n",
        "\n",
        "### Document Your Observations:\n",
        "1. **Citi Bike Data Structure**: \n",
        "   - Number of records: **247,584** (full year 2016)\n",
        "   - Columns: **15 columns** (Trip Duration, Start/Stop Time, Station info, Bike ID, User Type, Birth Year, Gender)\n",
        "   - Data types: Mix of int64, float64, and object types\n",
        "   - Missing values: **Birth Year (7.67% missing)**, **User Type (0.15% missing)**\n",
        "\n",
        "2. **Weather Data Structure**:\n",
        "   - Number of records: **366** (daily data for 2016)\n",
        "   - Columns: **16 columns** (Station info, Date, Temperature, Precipitation, Wind, etc.)\n",
        "   - Data types: Mix of int64, float64, and object types\n",
        "   - Missing values: **PGTM (100% missing)**, **TSUN (100% missing)**, **WDF5/WSF5 (0.55% missing)**\n",
        "\n",
        "3. **Data Quality Issues Found**:\n",
        "   - [x] Missing birth year data (7.67% - significant for demographic analysis)\n",
        "   - [x] Extreme trip durations (max: 16,329,810 seconds = ~189 days!)\n",
        "   - [x] Missing weather data (PGTM, TSUN completely missing)\n",
        "   - [x] Other issues: **User Type missing for some records**\n",
        "\n",
        "4. **Key Insights Discovered**:\n",
        "   - **Date Coverage**: Perfect overlap (2016-01-01 to 2016-12-31) for both datasets\n",
        "   - **User Distribution**: 93.7% Subscribers vs 6.3% Customers\n",
        "   - **Gender Distribution**: 1=Male (71.5%), 2=Female (20.4%), 0=Unknown (8.0%)\n",
        "   - **Trip Duration**: Median ~6.5 minutes, but extreme outliers exist\n",
        "   - **Weather Variables**: Temperature (TAVG, TMAX, TMIN), Precipitation (PRCP), Wind (AWND, WSF2/WSF5)\n",
        "\n",
        "### Key Questions for Phase 2:\n",
        "- How will you handle missing birth year data? (7.67% missing)\n",
        "- Should you filter out extreme trip durations? (some trips > 100 days!)\n",
        "- How will you align weather data with bike trips? (daily weather vs hourly trips)\n",
        "- What time zones are you working with? (appears to be EST/EDT)\n",
        "\n",
        "### Next Steps:\n",
        "- [x] Complete Phase 1 exploration\n",
        "- [ ] Move to Phase 2: Data Cleaning & Validation\n",
        "- [ ] Create data cleaning pipeline\n",
        "- [ ] Handle extreme trip duration outliers\n",
        "- [ ] Decide on missing data strategy\n",
        "\n",
        "---\n",
        "\n",
        "**Great work!** You've successfully identified the key data quality issues and understand both datasets well. The perfect date overlap is excellent for analysis!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Codecademy ML",
      "language": "python",
      "name": "codeacademy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
